{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eae5f51",
   "metadata": {},
   "source": [
    "#### Byte-Pait Encoding (BPE)\n",
    "##### Baixando as dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9abe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b38c57",
   "metadata": {},
   "source": [
    "##### Baixando córpus do Wikepedia para treinar o tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e2f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Define o URL do arquivo a ser baixado\n",
    "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip'\n",
    "\n",
    "# Define o nome do arquivo local\n",
    "filename = 'wikitext-103-raw-v1.zip'\n",
    "\n",
    "# Faz o download do arquivo usando urllib\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Abre o arquivo ZIP e extrai o conteúdo\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b8662",
   "metadata": {},
   "source": [
    "##### iniciando o tokoneziador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51aa9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unnk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e626c1e",
   "metadata": {},
   "source": [
    "##### iniciando o mòdulo de Treinamento\n",
    "##### Define-se um vocabulário desejado com 30000 símbolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "                    vocab_size = 30000,\n",
    "                     min_frequency=0,\n",
    "                     continuing_subword_prefix=\"##\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa988700",
   "metadata": {},
   "source": [
    "##### Definindo pré-tokenizador por espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af5e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04f65c",
   "metadata": {},
   "source": [
    "##### Salvando o Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b5de3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e92e8b",
   "metadata": {},
   "source": [
    "##### Carregando o Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528bd9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502ab65",
   "metadata": {},
   "source": [
    "##### Tokenizando textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5693c1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No',\n",
       " 'in',\n",
       " '##í',\n",
       " '##c',\n",
       " '##io',\n",
       " 'de',\n",
       " 'j',\n",
       " '##ul',\n",
       " '##ho',\n",
       " ',',\n",
       " 'em',\n",
       " 'u',\n",
       " '##m',\n",
       " 'cal',\n",
       " '##or',\n",
       " 'su',\n",
       " '##f',\n",
       " '##oc',\n",
       " '##ante',\n",
       " ',',\n",
       " 'à',\n",
       " '##s',\n",
       " 'qu',\n",
       " '##at',\n",
       " '##ro',\n",
       " 'hor',\n",
       " '##as',\n",
       " 'da',\n",
       " 't',\n",
       " '##ard',\n",
       " '##e',\n",
       " ',',\n",
       " 'u',\n",
       " '##m',\n",
       " 'j',\n",
       " '##ov',\n",
       " '##em',\n",
       " 's',\n",
       " '##ai',\n",
       " '##u',\n",
       " 'do',\n",
       " 'qu',\n",
       " '##art',\n",
       " '##in',\n",
       " '##ho',\n",
       " 'al',\n",
       " '##ug',\n",
       " '##ado',\n",
       " 'de',\n",
       " 'u',\n",
       " '##ma',\n",
       " 'cas',\n",
       " '##a',\n",
       " 'de',\n",
       " 'do',\n",
       " '##is',\n",
       " 'and',\n",
       " '##ares',\n",
       " 'no',\n",
       " 'bec',\n",
       " '##o',\n",
       " 'de',\n",
       " 'S',\n",
       " '.',\n",
       " 'e',\n",
       " 'c',\n",
       " '##amin',\n",
       " '##hou',\n",
       " 'lent',\n",
       " '##ament',\n",
       " '##e',\n",
       " ',',\n",
       " 'com',\n",
       " '##o',\n",
       " 'se',\n",
       " 'est',\n",
       " '##iv',\n",
       " '##ess',\n",
       " '##e',\n",
       " 'hes',\n",
       " '##it',\n",
       " '##ando',\n",
       " ',',\n",
       " 'em',\n",
       " 'dire',\n",
       " '##ç',\n",
       " '##ão',\n",
       " 'à',\n",
       " 'p',\n",
       " '##on',\n",
       " '##te',\n",
       " 'de',\n",
       " 'K',\n",
       " '.',\n",
       " 'Ele',\n",
       " 'era',\n",
       " 'u',\n",
       " '##m',\n",
       " 'j',\n",
       " '##ov',\n",
       " '##em',\n",
       " 'de',\n",
       " 'cer',\n",
       " '##ca',\n",
       " 'de',\n",
       " 'vin',\n",
       " '##te',\n",
       " 'e',\n",
       " 'c',\n",
       " '##inc',\n",
       " '##o',\n",
       " 'an',\n",
       " '##os',\n",
       " ',',\n",
       " 'mag',\n",
       " '##ro',\n",
       " 'e',\n",
       " 'ba',\n",
       " '##ix',\n",
       " '##o',\n",
       " ',',\n",
       " 'de',\n",
       " 'cab',\n",
       " '##el',\n",
       " '##os',\n",
       " 'cast',\n",
       " '##an',\n",
       " '##h',\n",
       " '##os',\n",
       " ',',\n",
       " 'r',\n",
       " '##ost',\n",
       " '##o',\n",
       " 'p',\n",
       " '##ál',\n",
       " '##ido',\n",
       " 'e',\n",
       " 'que',\n",
       " 'par',\n",
       " '##ec',\n",
       " '##ia',\n",
       " 'do',\n",
       " '##ente',\n",
       " '.',\n",
       " 'V',\n",
       " '##est',\n",
       " '##ia',\n",
       " 'u',\n",
       " '##ma',\n",
       " 'j',\n",
       " '##a',\n",
       " '##qu',\n",
       " '##eta',\n",
       " 'de',\n",
       " 'ver',\n",
       " '##ão',\n",
       " 'e',\n",
       " 'u',\n",
       " '##m',\n",
       " 'bon',\n",
       " '##é',\n",
       " 'de',\n",
       " 'pal',\n",
       " '##ha',\n",
       " 'vel',\n",
       " '##ho',\n",
       " ',',\n",
       " 'e',\n",
       " 'car',\n",
       " '##reg',\n",
       " '##ava',\n",
       " 'u',\n",
       " '##ma',\n",
       " 'm',\n",
       " '##och',\n",
       " '##ila',\n",
       " 'grand',\n",
       " '##e',\n",
       " 'nas',\n",
       " 'cost',\n",
       " '##as',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"No início de julho, em um calor sufocante, às quatro horas da tarde, um jovem saiu do quartinho alugado de uma casa de dois andares no beco de S. e caminhou lentamente, como se estivesse hesitando, em direção à ponte de K. Ele era um jovem de cerca de vinte e cinco anos, magro e baixo, de cabelos castanhos, rosto pálido e que parecia doente. Vestia uma jaqueta de verão e um boné de palha velho, e carregava uma mochila grande nas costas.\"\n",
    "output = tokenizer.encode(texto)\n",
    "output.tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
